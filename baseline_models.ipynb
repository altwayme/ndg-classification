{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef854ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── ⓵ Загрузка и «распаковка» JSON ─────────────────────────────────────────────\n",
    "with open('articles.json', 'r', encoding='utf-8') as f:\n",
    "    raw = json.load(f)\n",
    "# сразу распаковываем только нужные поля:\n",
    "df = pd.json_normalize(\n",
    "    [{\"id\": k, **v[\"data\"]} for k, v in raw.items()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5b3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── ⓶ Чистка и фильтрация меток ───────────────────────────────────────────────\n",
    "def clean_classification(x):\n",
    "    if isinstance(x, str) and \"Неверный формат ответа\" in x:\n",
    "        return []\n",
    "    return x if isinstance(x, list) else []\n",
    "\n",
    "df['classification'] = df['classification'].apply(clean_classification)\n",
    "df = df[df['classification'].map(len) > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc6cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Лемматизация: 100%|██████████| 4558/4558 [12:53<00:00,  5.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# ─── ⓷ Препроцессинг + лемматизация (с кешем) ─────────────────────────────────\n",
    "cache_path = 'text_lemmatized_cache.csv'\n",
    "if os.path.exists(cache_path):\n",
    "    df = pd.read_csv(cache_path, index_col=0)\n",
    "else:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('russian'))\n",
    "    morph = MorphAnalyzer()\n",
    "\n",
    "    def preprocess_text(text: str) -> str:\n",
    "        text = str(text)\n",
    "        # убираем HTML, невидимые символы и нормализуем\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "        text = text.replace('Ё','Е').replace('ё','е')\n",
    "        text = re.sub(r'[^A-Za-z0-9А-Яа-яЕе\\-\\.,:/%]', ' ', text)\n",
    "        text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "        # токенизация + стоп-слова\n",
    "        tokens = [t for t in text.split() if t.lower() not in stop_words and len(t)>2]\n",
    "        # лемматизация\n",
    "        return ' '.join(morph.parse(t)[0].normal_form for t in tokens)\n",
    "\n",
    "    tqdm.pandas(desc=\"Лемматизация\")\n",
    "    df['text_lemmatized'] = df['text'].progress_apply(preprocess_text)\n",
    "    df.to_csv(cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1115d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── ⓸ Кодирование меток ────────────────────────────────────────────────────────\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df['classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58941a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── ⓹ Стратифицированный сплит по мульти-лейблам ───────────────────────────────\n",
    "texts = df['text_lemmatized'].values\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(msss.split(texts, Y))\n",
    "X_train_texts, X_test_texts = texts[train_idx], texts[test_idx]\n",
    "Y_train, Y_test = Y[train_idx], Y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def6e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── ⓺ Pipeline: TF-IDF + One-vs-Rest LogisticRegression ───────────────────────\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', OneVsRestClassifier(\n",
    "        LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24b4e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    }
   ],
   "source": [
    "# ─── ⓻ GridSearch по параметру C ───────────────────────────────────────────────\n",
    "param_grid = {\n",
    "    'clf__estimator__C': [0.1, 1, 10],\n",
    "}\n",
    "grid_tfidf = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_micro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_tfidf.fit(X_train_texts, Y_train)\n",
    "best_tfidf = grid_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609a49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline: TF-IDF + One-vs-Rest LogisticRegression ===\n",
      "Лучший C: 10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.63      0.51       102\n",
      "           1       0.84      0.92      0.88       327\n",
      "           2       0.83      0.90      0.86       177\n",
      "           3       0.78      0.83      0.80       162\n",
      "           4       0.86      0.84      0.85        45\n",
      "           5       0.67      0.82      0.74       133\n",
      "           6       0.40      0.59      0.48        29\n",
      "           7       0.59      0.62      0.60       154\n",
      "\n",
      "   micro avg       0.72      0.81      0.76      1129\n",
      "   macro avg       0.67      0.77      0.72      1129\n",
      "weighted avg       0.73      0.81      0.77      1129\n",
      " samples avg       0.77      0.86      0.79      1129\n",
      "\n",
      "ROC-AUC (micro): 0.9620281429685595\n",
      "ROC-AUC (macro): 0.9475017048701386\n"
     ]
    }
   ],
   "source": [
    "# ─── ⓼ Оценка на тесте ─────────────────────────────────────────────────────────\n",
    "Y_pred  = best_tfidf.predict(X_test_texts)\n",
    "Y_proba = best_tfidf.predict_proba(X_test_texts)\n",
    "\n",
    "print(\"\\n=== Baseline: TF-IDF + One-vs-Rest LogisticRegression ===\")\n",
    "print(\"Лучший C:\", grid_tfidf.best_params_['clf__estimator__C'])\n",
    "print(classification_report(\n",
    "    Y_test, Y_pred,\n",
    "    target_names=[str(c) for c in mlb.classes_],\n",
    "    zero_division=0\n",
    "))\n",
    "print(\"ROC-AUC (micro):\", roc_auc_score(Y_test,  Y_proba, average='micro'))\n",
    "print(\"ROC-AUC (macro):\", roc_auc_score(Y_test,  Y_proba, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e14268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Multi-label One-vs-One (через MultiOutput) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.63      0.51       102\n",
      "           1       0.84      0.93      0.88       327\n",
      "           2       0.82      0.90      0.86       177\n",
      "           3       0.78      0.83      0.81       162\n",
      "           4       0.87      0.87      0.87        45\n",
      "           5       0.67      0.84      0.75       133\n",
      "           6       0.39      0.59      0.47        29\n",
      "           7       0.60      0.62      0.61       154\n",
      "\n",
      "   micro avg       0.72      0.82      0.76      1129\n",
      "   macro avg       0.67      0.78      0.72      1129\n",
      "weighted avg       0.73      0.82      0.77      1129\n",
      " samples avg       0.77      0.86      0.79      1129\n",
      "\n",
      "ROC-AUC (micro): 0.9619188114612541\n",
      "ROC-AUC (macro): 0.9472226455809138\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# ⓵ Сначала векторизуем тексты (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_texts)\n",
    "X_test_vec  = vectorizer.transform(   X_test_texts)\n",
    "\n",
    "# ⓶ Строим MultiOutput + OVO\n",
    "ovo_multi = MultiOutputClassifier(\n",
    "    OneVsOneClassifier(\n",
    "        LogisticRegression(\n",
    "            class_weight='balanced',\n",
    "            C=grid_tfidf.best_params_['clf__estimator__C'],  # лучший C из GridSearch\n",
    "            max_iter=1000,\n",
    "            solver='liblinear'  # для бинарной задачи liblinear хорошо\n",
    "        ),\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ovo_multi.fit(X_train_vec, Y_train)\n",
    "Y_pred_ovo = ovo_multi.predict(X_test_vec)\n",
    "\n",
    "# ⓷ Собираем вероятности «метка=1» у каждого внутреннего классификатора\n",
    "proba_list = []\n",
    "for ovo_clf in ovo_multi.estimators_:\n",
    "    # ovo_clf.estimators_ – список внутренних бинарных клаcсификаторов,\n",
    "    # здесь он длины 1, потому что наша задача 0 vs 1:\n",
    "    base = ovo_clf.estimators_[0]\n",
    "    proba = base.predict_proba(X_test_vec)[:, 1]\n",
    "    proba_list.append(proba)\n",
    "\n",
    "Y_proba_ovo = np.vstack(proba_list).T  # shape = (n_samples, n_labels)\n",
    "\n",
    "# ⓸ Метрики\n",
    "print(\"\\n=== Multi-label One-vs-One (через MultiOutput) ===\")\n",
    "print(classification_report(\n",
    "    Y_test,\n",
    "    Y_pred_ovo,\n",
    "    target_names=[str(c) for c in mlb.classes_],\n",
    "    zero_division=0\n",
    "))\n",
    "print(\"ROC-AUC (micro):\", roc_auc_score(Y_test, Y_proba_ovo, average='micro'))\n",
    "print(\"ROC-AUC (macro):\", roc_auc_score(Y_test, Y_proba_ovo, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "159498d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ClassifierChain LogisticRegression ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.45      0.43       102\n",
      "           1       0.84      0.92      0.88       327\n",
      "           2       0.88      0.84      0.86       177\n",
      "           3       0.79      0.78      0.79       162\n",
      "           4       0.85      0.78      0.81        45\n",
      "           5       0.67      0.82      0.74       133\n",
      "           6       0.31      0.52      0.38        29\n",
      "           7       0.57      0.62      0.59       154\n",
      "\n",
      "   micro avg       0.72      0.78      0.75      1129\n",
      "   macro avg       0.66      0.72      0.68      1129\n",
      "weighted avg       0.73      0.78      0.75      1129\n",
      " samples avg       0.77      0.81      0.77      1129\n",
      "\n",
      "ROC-AUC (micro): 0.9277832412342865\n",
      "ROC-AUC (macro): 0.8952148897994732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# 1) Забираем обученный TF-IDF из вашего GridSearch-pipeline\n",
    "tfidf = best_tfidf.named_steps['tfidf']\n",
    "\n",
    "# 2) Преобразуем тексты в векторы\n",
    "X_train_vec = tfidf.transform(X_train_texts)\n",
    "X_test_vec  = tfidf.transform(X_test_texts)\n",
    "\n",
    "# 3) Инициализируем ClassifierChain с теми же LR-параметрами\n",
    "chain = ClassifierChain(\n",
    "    base_estimator=LogisticRegression(\n",
    "        C=grid_tfidf.best_params_['clf__estimator__C'],\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    order='random',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4) Обучаем цепочку\n",
    "chain.fit(X_train_vec, Y_train)\n",
    "\n",
    "# 5) Предсказываем метки\n",
    "Y_pred_chain = chain.predict(X_test_vec)\n",
    "\n",
    "# 6) Берём вероятности «1» для каждой метки\n",
    "# Если ваша версия sklearn поддерживает predict_proba у ClassifierChain:\n",
    "try:\n",
    "    Y_proba_chain = chain.predict_proba(X_test_vec)\n",
    "except AttributeError:\n",
    "    # Иначе: вручную последовательно наращиваем «предсказанные» фичи\n",
    "    from scipy.sparse import hstack\n",
    "    X_ext = X_test_vec\n",
    "    proba_list = []\n",
    "    for est in chain.estimators_:\n",
    "        p = est.predict_proba(X_ext)[:, 1]\n",
    "        proba_list.append(p)\n",
    "        # бинарные предсказания для следующего шага\n",
    "        y_bin = est.predict(X_ext).reshape(-1, 1)\n",
    "        X_ext = hstack([X_ext, y_bin])\n",
    "    Y_proba_chain = np.vstack(proba_list).T\n",
    "\n",
    "# 7) Выводим те же метрики\n",
    "print(\"\\n=== ClassifierChain LogisticRegression ===\")\n",
    "print(classification_report(\n",
    "    Y_test, Y_pred_chain,\n",
    "    target_names=[str(c) for c in mlb.classes_],\n",
    "    zero_division=0\n",
    "))\n",
    "print(\"ROC-AUC (micro):\", roc_auc_score(Y_test, Y_proba_chain, average='micro'))\n",
    "print(\"ROC-AUC (macro):\", roc_auc_score(Y_test, Y_proba_chain, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1f2e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline: word+char TF–IDF + OneVsRest(LogReg) ===\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.75      0.48       102\n",
      "           1       0.83      0.92      0.87       327\n",
      "           2       0.82      0.95      0.88       177\n",
      "           3       0.77      0.88      0.82       162\n",
      "           4       0.65      0.91      0.76        45\n",
      "           5       0.63      0.87      0.73       133\n",
      "           6       0.14      0.79      0.23        29\n",
      "           7       0.56      0.74      0.64       154\n",
      "\n",
      "   micro avg       0.62      0.87      0.72      1129\n",
      "   macro avg       0.59      0.85      0.68      1129\n",
      "weighted avg       0.69      0.87      0.76      1129\n",
      " samples avg       0.72      0.90      0.77      1129\n",
      "\n",
      "ROC-AUC (micro): 0.9543\n",
      "ROC-AUC (macro): 0.9489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline        import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model   import LogisticRegression\n",
    "from sklearn.multiclass     import OneVsRestClassifier\n",
    "from sklearn.metrics        import classification_report, roc_auc_score\n",
    "\n",
    "# 1) Собираем комбинированный TF–IDF\n",
    "tfidf_wc = FeatureUnion([\n",
    "    (\"word\", TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=(1,2),\n",
    "        max_features=10000,\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    "    )),\n",
    "    (\"char\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3,5),\n",
    "        max_features=5000\n",
    "    )),\n",
    "])\n",
    "\n",
    "# 2) Собираем Pipeline\n",
    "wordchar_pipeline = Pipeline([\n",
    "    (\"tfidf\", tfidf_wc),\n",
    "    (\"clf\", OneVsRestClassifier(\n",
    "        LogisticRegression(\n",
    "            solver=\"saga\",      # хорошо работает с большими sparse-разреженными фичами\n",
    "            penalty=\"l2\",\n",
    "            C=1.0,              # можно покрутить\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=1000\n",
    "        ),\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "# 3) Обучение\n",
    "wordchar_pipeline.fit(X_train_texts, Y_train)\n",
    "best_wordchar = wordchar_pipeline\n",
    "\n",
    "# 4) Предсказание\n",
    "Y_pred  = wordchar_pipeline.predict(X_test_texts)\n",
    "Y_proba = wordchar_pipeline.predict_proba(X_test_texts)\n",
    "\n",
    "# 5) Оценка\n",
    "print(\"=== Baseline: word+char TF–IDF + OneVsRest(LogReg) ===\\n\")\n",
    "print(classification_report(\n",
    "    Y_test, Y_pred, \n",
    "    target_names=[str(c) for c in mlb.classes_],\n",
    "    zero_division=0\n",
    "))\n",
    "print(f\"ROC-AUC (micro): {roc_auc_score(Y_test, Y_proba, average='micro'):.4f}\")\n",
    "print(f\"ROC-AUC (macro): {roc_auc_score(Y_test, Y_proba, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f4e183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline: word+char TF–IDF + OneVsRest(LogReg) ===\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.75      0.48       102\n",
      "           1       0.83      0.92      0.87       327\n",
      "           2       0.82      0.95      0.88       177\n",
      "           3       0.77      0.88      0.82       162\n",
      "           4       0.65      0.91      0.76        45\n",
      "           5       0.63      0.87      0.73       133\n",
      "           6       0.14      0.79      0.23        29\n",
      "           7       0.56      0.74      0.64       154\n",
      "\n",
      "   micro avg       0.62      0.87      0.72      1129\n",
      "   macro avg       0.59      0.85      0.68      1129\n",
      "weighted avg       0.69      0.87      0.76      1129\n",
      " samples avg       0.72      0.90      0.77      1129\n",
      "\n",
      "ROC-AUC (micro): 0.9543\n",
      "ROC-AUC (macro): 0.9489\n"
     ]
    }
   ],
   "source": [
    "# 5) Оценка\n",
    "print(\"=== Baseline: word+char TF–IDF + OneVsRest(LogReg) ===\\n\")\n",
    "print(classification_report(\n",
    "    Y_test, Y_pred, \n",
    "    target_names=[str(c) for c in mlb.classes_],\n",
    "    zero_division=0\n",
    "))\n",
    "print(f\"ROC-AUC (micro): {roc_auc_score(Y_test, Y_proba, average='micro'):.4f}\")\n",
    "print(f\"ROC-AUC (macro): {roc_auc_score(Y_test, Y_proba, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1681480c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "\n",
      "=== Baseline: BoW + One-vs-Rest LogisticRegression ===\n",
      "Best C: 0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.49      0.46       102\n",
      "           1       0.83      0.88      0.85       327\n",
      "           2       0.83      0.87      0.85       177\n",
      "           3       0.77      0.75      0.76       162\n",
      "           4       0.85      0.76      0.80        45\n",
      "           5       0.73      0.77      0.75       133\n",
      "           6       0.43      0.41      0.42        29\n",
      "           7       0.59      0.51      0.55       154\n",
      "\n",
      "   micro avg       0.73      0.74      0.74      1129\n",
      "   macro avg       0.68      0.68      0.68      1129\n",
      "weighted avg       0.73      0.74      0.74      1129\n",
      " samples avg       0.74      0.79      0.75      1129\n",
      "\n",
      "ROC-AUC (micro): 0.9321869571581892\n",
      "ROC-AUC (macro): 0.9149329291942099\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Changed from TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "# Assuming all preprocessing steps (loading, cleaning, lemmatization, label encoding, and train-test split)\n",
    "# are the same as in your original code, up to the pipeline definition.\n",
    "\n",
    "# ─── ⓺ Pipeline: BoW + One-vs-Rest LogisticRegression ───────────────────────\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(max_features=5000)),  # BoW instead of TF-IDF\n",
    "    ('clf', OneVsRestClassifier(\n",
    "        LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    )),\n",
    "])\n",
    "\n",
    "# ─── ⓻ GridSearch over C parameter ──────────────────────────────────────────\n",
    "param_grid = {\n",
    "    'clf__estimator__C': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "grid_bow = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_micro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_bow.fit(X_train_texts, Y_train)\n",
    "best_bow = grid_bow.best_estimator_       # keep a separate copy for BoW\n",
    "\n",
    "\n",
    "# ─── ⓼ Evaluate on test set ────────────────────────────────────────────────\n",
    "Y_pred = best_bow.predict(X_test_texts)\n",
    "Y_proba = best_bow.predict_proba(X_test_texts)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Baseline: BoW + One-vs-Rest LogisticRegression ===\")\n",
    "print(\"Best C:\", grid_bow.best_params_['clf__estimator__C'])\n",
    "print(classification_report(\n",
    "    Y_test, Y_pred,\n",
    "    target_names=[str(c) for c in mlb.classes_],\n",
    "    zero_division=0\n",
    "))\n",
    "print(\"ROC-AUC (micro):\", roc_auc_score(Y_test, Y_proba, average='micro'))\n",
    "print(\"ROC-AUC (macro):\", roc_auc_score(Y_test, Y_proba, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200a829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Лемматизация: 100%|██████████| 400/400 [01:13<00:00,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 0. paths & helper functions\n",
    "# -----------------------------------------------------------\n",
    "MANUAL_PATH = r\"400.json\"\n",
    "\n",
    "def to_int_list(lbls):\n",
    "    \"\"\"Safely cast JSON labels to a list[int].\"\"\"\n",
    "    return [int(x) for x in lbls] if isinstance(lbls, list) else []\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"⇢ the *same* cleaning / lemmatisation routine you used before.\"\"\"\n",
    "    text = str(text).replace('Ё', 'Е').replace('ё', 'е')\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9А-Яа-яЕе\\-\\.,:/%]', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "\n",
    "    tokens = [\n",
    "        t for t in text.split()\n",
    "        if t.lower() not in stop_words and len(t) > 2\n",
    "    ]\n",
    "    return ' '.join(morph.parse(t)[0].normal_form for t in tokens)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. load & prepare the gold validation data\n",
    "# -----------------------------------------------------------\n",
    "with open(MANUAL_PATH, encoding='utf-8') as f:\n",
    "    gold_raw = json.load(f)\n",
    "\n",
    "gold_df = pd.json_normalize(\n",
    "    [{\"id\": k, **v[\"data\"]} for k, v in gold_raw.items()]\n",
    ")\n",
    "\n",
    "# take ONLY rows that have non‑empty manual_labels\n",
    "gold_df = gold_df[gold_df[\"manual_labels\"].map(bool)].copy()\n",
    "\n",
    "gold_df[\"manual_labels\"] = gold_df[\"manual_labels\"].apply(to_int_list)\n",
    "gold_df[\"text_lemmatized\"] = gold_df[\"text\"].progress_apply(preprocess_text)\n",
    "\n",
    "# binarise with the SAME MultiLabelBinarizer that is already fitted\n",
    "Y_val = mlb.transform(gold_df[\"manual_labels\"])\n",
    "X_val_texts = gold_df[\"text_lemmatized\"].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "724a7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2. vectorise once for the models that need raw TF‑IDF / BoW\n",
    "# -----------------------------------------------------------\n",
    "X_val_tfidf   = best_tfidf.named_steps['tfidf'].transform(X_val_texts)\n",
    "X_val_bow     = best_bow.named_steps['bow'].transform(X_val_texts)\n",
    "X_val_tfidfwc = best_wordchar.named_steps['tfidf'].transform(X_val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e838d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf', TfidfVectorizer(max_features=5000)),\n",
      "                ('clf',\n",
      "                 OneVsRestClassifier(estimator=LogisticRegression(C=10,\n",
      "                                                                  class_weight='balanced',\n",
      "                                                                  max_iter=1000)))])\n",
      "Pipeline(steps=[('bow', CountVectorizer(max_features=5000)),\n",
      "                ('clf',\n",
      "                 OneVsRestClassifier(estimator=LogisticRegression(C=0.1,\n",
      "                                                                  class_weight='balanced',\n",
      "                                                                  max_iter=1000)))])\n"
     ]
    }
   ],
   "source": [
    "print(best_tfidf)   # should show ('tfidf', TfidfVectorizer(...))\n",
    "print(best_bow)     # should show ('bow',   CountVectorizer(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccfd659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3. evaluation helper\n",
    "# -----------------------------------------------------------\n",
    "# Compute probabilities for OvO on the validation set\n",
    "proba_list_val = []\n",
    "for ovo_clf in ovo_multi.estimators_:\n",
    "    base = ovo_clf.estimators_[0]\n",
    "    proba = base.predict_proba(X_val_tfidf)[:, 1]\n",
    "    proba_list_val.append(proba)\n",
    "Y_proba_ovo_val = np.vstack(proba_list_val).T  # shape = (n_samples, n_labels)\n",
    "\n",
    "# Modified evaluate function to accept precomputed probabilities\n",
    "def evaluate(name, estimator, X_mat, Y_proba_precomputed=None):\n",
    "    \"\"\"Prints report & ROC‑AUC for a fitted *multi‑label* estimator.\"\"\"\n",
    "    Y_pred = estimator.predict(X_mat)\n",
    "    \n",
    "    print(f\"\\n=== {name} on manual validation set ===\")\n",
    "    print(classification_report(\n",
    "        Y_val, Y_pred,\n",
    "        target_names=[str(c) for c in mlb.classes_],\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    # Use precomputed probabilities if provided, otherwise try predict_proba\n",
    "    if Y_proba_precomputed is not None:\n",
    "        Y_proba = Y_proba_precomputed\n",
    "    else:\n",
    "        try:\n",
    "            Y_proba = estimator.predict_proba(X_mat)\n",
    "        except AttributeError:\n",
    "            Y_proba = None\n",
    "\n",
    "    if Y_proba is not None:\n",
    "        print(\"ROC‑AUC (micro):\", roc_auc_score(Y_val, Y_proba, average='micro'))\n",
    "        print(\"ROC‑AUC (macro):\", roc_auc_score(Y_val, Y_proba, average='macro'))\n",
    "    else:\n",
    "        print(\"ROC-AUC: Not available (predict_proba not supported)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e57176ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF‑IDF  + OvR (C = 10) on manual validation set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.62      0.54        50\n",
      "           1       0.72      0.72      0.72        80\n",
      "           2       0.79      0.75      0.77        64\n",
      "           3       0.72      0.71      0.72        83\n",
      "           4       0.92      0.82      0.87        55\n",
      "           5       0.75      0.81      0.78       108\n",
      "           6       0.81      0.69      0.75        72\n",
      "           7       0.68      0.70      0.69       131\n",
      "\n",
      "   micro avg       0.72      0.73      0.73       643\n",
      "   macro avg       0.73      0.73      0.73       643\n",
      "weighted avg       0.73      0.73      0.73       643\n",
      " samples avg       0.74      0.79      0.73       643\n",
      "\n",
      "ROC‑AUC (micro): 0.9251546846974518\n",
      "ROC‑AUC (macro): 0.9185668463947259\n",
      "\n",
      "=== TF‑IDF + OvO on manual validation set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.60      0.54        50\n",
      "           1       0.72      0.72      0.72        80\n",
      "           2       0.79      0.75      0.77        64\n",
      "           3       0.71      0.71      0.71        83\n",
      "           4       0.92      0.82      0.87        55\n",
      "           5       0.75      0.81      0.78       108\n",
      "           6       0.81      0.71      0.76        72\n",
      "           7       0.68      0.69      0.69       131\n",
      "\n",
      "   micro avg       0.72      0.73      0.73       643\n",
      "   macro avg       0.73      0.73      0.73       643\n",
      "weighted avg       0.73      0.73      0.73       643\n",
      " samples avg       0.74      0.78      0.73       643\n",
      "\n",
      "ROC‑AUC (micro): 0.9246535141845245\n",
      "ROC‑AUC (macro): 0.9180171798877163\n",
      "\n",
      "=== TF‑IDF  + Classifier‑Chain on manual validation set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.44      0.42        50\n",
      "           1       0.72      0.72      0.72        80\n",
      "           2       0.87      0.72      0.79        64\n",
      "           3       0.71      0.61      0.66        83\n",
      "           4       0.90      0.78      0.83        55\n",
      "           5       0.75      0.82      0.78       108\n",
      "           6       0.75      0.71      0.73        72\n",
      "           7       0.66      0.68      0.67       131\n",
      "\n",
      "   micro avg       0.71      0.70      0.71       643\n",
      "   macro avg       0.72      0.69      0.70       643\n",
      "weighted avg       0.72      0.70      0.71       643\n",
      " samples avg       0.73      0.74      0.71       643\n",
      "\n",
      "ROC‑AUC (micro): 0.8975532052712921\n",
      "ROC‑AUC (macro): 0.8846715162845504\n",
      "\n",
      "=== word+char TF‑IDF + OvR on manual validation set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.74      0.56        50\n",
      "           1       0.71      0.76      0.73        80\n",
      "           2       0.75      0.80      0.77        64\n",
      "           3       0.61      0.72      0.66        83\n",
      "           4       0.74      0.91      0.81        55\n",
      "           5       0.71      0.87      0.78       108\n",
      "           6       0.49      0.94      0.64        72\n",
      "           7       0.65      0.81      0.72       131\n",
      "\n",
      "   micro avg       0.63      0.82      0.71       643\n",
      "   macro avg       0.64      0.82      0.71       643\n",
      "weighted avg       0.64      0.82      0.72       643\n",
      " samples avg       0.69      0.87      0.73       643\n",
      "\n",
      "ROC‑AUC (micro): 0.9185464108831852\n",
      "ROC‑AUC (macro): 0.9208808369568801\n",
      "\n",
      "=== BoW + OvR (clf only, C = 0.1) on manual validation set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.42      0.42        50\n",
      "           1       0.70      0.71      0.71        80\n",
      "           2       0.77      0.73      0.75        64\n",
      "           3       0.70      0.63      0.66        83\n",
      "           4       0.90      0.82      0.86        55\n",
      "           5       0.75      0.76      0.75       108\n",
      "           6       0.87      0.57      0.69        72\n",
      "           7       0.68      0.56      0.61       131\n",
      "\n",
      "   micro avg       0.72      0.65      0.68       643\n",
      "   macro avg       0.72      0.65      0.68       643\n",
      "weighted avg       0.73      0.65      0.68       643\n",
      " samples avg       0.71      0.71      0.68       643\n",
      "\n",
      "ROC‑AUC (micro): 0.8893471463387487\n",
      "ROC‑AUC (macro): 0.8791709934882096\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4. run every classical model\n",
    "# -----------------------------------------------------------\n",
    "# 4.1 TF‑IDF  + OvR (best from GridSearch)\n",
    "evaluate(\"TF‑IDF  + OvR (C = {})\".format(\n",
    "    best_tfidf.named_steps['clf'].estimator.C), best_tfidf, X_val_texts)\n",
    "\n",
    "# 4.2 TF‑IDF  + OvO\n",
    "evaluate(\"TF‑IDF + OvO\", ovo_multi, X_val_tfidf, Y_proba_precomputed=Y_proba_ovo_val)\n",
    "\n",
    "# 4.3 TF‑IDF  + Classifier Chain\n",
    "evaluate(\"TF‑IDF  + Classifier‑Chain\", chain, X_val_tfidf)\n",
    "\n",
    "# 4.4 word+char TF‑IDF + OvR\n",
    "evaluate(\"word+char TF‑IDF + OvR\", wordchar_pipeline, X_val_texts)\n",
    "\n",
    "# 4.5 Bag‑of‑Words + OvR\n",
    "bow_clf = best_bow.named_steps['clf']          # only the classifier\n",
    "evaluate(\"BoW + OvR (clf only, C = {})\".format(\n",
    "         bow_clf.estimator.C),\n",
    "         bow_clf,\n",
    "         X_val_bow)            # <-- csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01193ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38c3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
